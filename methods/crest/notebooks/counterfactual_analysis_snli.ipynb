{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d30063c-4ce0-42e1-8fdb-dc9341dfff36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textdistance\n",
    "import torch\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d9dd1ba-5eae-4f34-bcca-e5b3ac74f77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "t5_tokenizer = AutoTokenizer.from_pretrained('t5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb1c3c90-6092-4093-a83e-46f757d8c025",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mtreviso/roberta-base-snli\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model = model.cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae937052-be36-43f7-ae47-a8b69b17e651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, tokenizer, inp):\n",
    "    enc = tokenizer(\n",
    "        inp, \n",
    "        padding=True,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    out = model(\n",
    "        input_ids=enc.input_ids.cuda(), \n",
    "        attention_mask=enc.attention_mask.cuda()\n",
    "    )\n",
    "    y_hat = out.logits.argmax(-1).item()\n",
    "    if y_hat == 1:\n",
    "        return out.logits.argsort(-1)[:, -2].item()\n",
    "    return y_hat\n",
    "\n",
    "def get_predictions(model, tokenizer, inputs, verbose=True):\n",
    "    model.eval()\n",
    "    outs = []\n",
    "    with torch.no_grad():\n",
    "        gen = tqdm(inputs) if verbose else inputs\n",
    "        for inp in gen:\n",
    "            outs.append(predict(model, tokenizer, inp))\n",
    "    return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "762e23a3-efb3-4b4f-acc4-8a820a44bd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim(text):\n",
    "    text = text.replace('</s>', ' </s> ')\n",
    "    text = re.sub(r'( </s>)+', ' </s>', text)\n",
    "    text = re.sub(r'\\ +', ' ', text).strip()\n",
    "    parts = text.split('</s>')\n",
    "    text = parts[0].strip() + ' [SEP] ' + parts[1].strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def decode_and_trim(text):\n",
    "    text = t5_tokenizer.decode(t5_tokenizer.convert_tokens_to_ids(text.strip().split()))\n",
    "    return trim(text)\n",
    "\n",
    "\n",
    "def read_edits(fname, valids_only=False, return_refs=False):\n",
    "    df = pd.read_csv(\n",
    "        fname, \n",
    "        sep='\\t', \n",
    "        usecols=['orig_texts', 'orig_labels', 'orig_predictions', 'orig_z', \n",
    "                 'edits_texts', 'edits_labels', 'edits_predictions', 'edits_z_pre', 'edits_z_pos']\n",
    "    )\n",
    "    if valids_only:\n",
    "        df = df[df['edits_labels'] == df['edits_predictions']]\n",
    "    edits = df['edits_texts'].map(decode_and_trim).tolist()\n",
    "    refs = df['orig_texts'].map(decode_and_trim).tolist()\n",
    "    try:\n",
    "        edits_labels = df['edits_labels'].map(int).tolist()\n",
    "        edits_preds = df['edits_predictions'].map(int).tolist()\n",
    "        refs_labels = df['orig_labels'].map(int).tolist()\n",
    "        refs_preds = df['orig_predictions'].map(int).tolist()\n",
    "    except:\n",
    "        label_map = {\"entailment\":0, \"neutral\":1, \"contradiction\":2}\n",
    "        edits_labels = df['edits_labels'].apply(label_map.__getitem__).tolist()\n",
    "        edits_preds = df['edits_predictions'].apply(label_map.__getitem__).tolist()\n",
    "        refs_labels = df['orig_labels'].apply(label_map.__getitem__).tolist()\n",
    "        refs_preds = df['orig_predictions'].apply(label_map.__getitem__).tolist()\n",
    "    return edits, edits_labels, edits_preds, refs, refs_labels, refs_preds\n",
    "\n",
    "\n",
    "def read_edits_mice(fname, use_last_search_step=False, valids_only=False):\n",
    "    \n",
    "    def get_mice_counterfactuals(df_mice):\n",
    "        # \"\"\" MiCE writes all edits that are found in Stage 2, \n",
    "        # but we only want to evaluate the smallest per input. \n",
    "        df_test = df_mice[df_mice['sorted_idx'] == 0]\n",
    "        # reset index\n",
    "        df_test = df_test.reset_index(drop=True)\n",
    "        df_test = df_test.sort_values(by='data_idx')\n",
    "        return df_test.reset_index(drop=True)\n",
    "\n",
    "    def get_mice_counterfactuals_max(df_mice):\n",
    "        # \"\"\" MiCE writes all edits that are found in Stage 2, \n",
    "        # but we only want to evaluate the longest per input. \n",
    "        df_test = df_mice.groupby('data_idx').last()\n",
    "        # reset index\n",
    "        df_test = df_test.reset_index(drop=True)\n",
    "        return df_test\n",
    "    \n",
    "    try:\n",
    "        df_mice = pd.read_csv(fname, delimiter='\\t')\n",
    "    except:\n",
    "        df_mice = pd.read_csv(fname, delimiter='\\t', lineterminator='\\n')\n",
    "    if not use_last_search_step:\n",
    "        df_mice_test = get_mice_counterfactuals(df_mice)\n",
    "    else:\n",
    "        df_mice_test = get_mice_counterfactuals_max(df_mice)\n",
    "    \n",
    "    label_map = {\"entailment\":0, \"neutral\":1, \"contradiction\":2, 0:0, 1:1, 2:2}\n",
    "    valid_rows = df_mice_test['contrast_label'].map(lambda x: x in label_map.keys())\n",
    "    df_mice_test = df_mice_test[valid_rows].reset_index(drop=True)\n",
    "    \n",
    "    if valids_only:\n",
    "        df_mice_test = df_mice_test[df_mice_test['contrast_label'] == df_mice_test['new_pred']]\n",
    "    \n",
    "    refs = df_mice_test['orig_input'].map(trim).tolist()\n",
    "    refs_labels = df_mice_test['gold_label'].apply(label_map.__getitem__).tolist()\n",
    "    refs_preds = df_mice_test['orig_pred'].apply(label_map.__getitem__).tolist()\n",
    "    \n",
    "    edits = df_mice_test['edited_input'].map(trim).tolist()\n",
    "    edits_labels = df_mice_test['contrast_label'].apply(label_map.__getitem__).tolist()\n",
    "    edits_preds = df_mice_test['new_pred'].apply(label_map.__getitem__).tolist()\n",
    "    \n",
    "    return edits, edits_labels, edits_preds, refs, refs_labels, refs_preds\n",
    "\n",
    "\n",
    "def get_tokenized_texts(texts):\n",
    "    return [' '.join(regexp_tokenize(text, '\\w+|\\$[\\d\\.]+|\\S+')) for text in texts]\n",
    "\n",
    "\n",
    "def dist_ratio(es, rs):\n",
    "    return np.mean([\n",
    "        textdistance.levenshtein.normalized_distance(e.split(), r.split())\n",
    "        for e, r in zip(es, rs)\n",
    "    ])\n",
    "\n",
    "\n",
    "def clean(text):\n",
    "    text = text.replace('[SEP]', '')\n",
    "    text = re.sub(r'\\ +', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def split_and_clean(text):\n",
    "    text = text.split('[SEP]')[1]\n",
    "    text = re.sub(r'\\ +', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def print_eval(filename, valids_only=False, use_last_search_step=False):\n",
    "    if 'mice' in filename:\n",
    "        edits, edits_labels, edits_preds, refs, refs_labels, refs_preds = read_edits_mice(\n",
    "            filename, use_last_search_step=use_last_search_step, valids_only=valids_only\n",
    "        )\n",
    "    else:\n",
    "        edits, edits_labels, edits_preds, refs, refs_labels, refs_preds = read_edits(\n",
    "            filename, valids_only=valids_only\n",
    "        )\n",
    "    \n",
    "    acc = accuracy_score(refs_labels, refs_preds)\n",
    "    f1 = f1_score(refs_labels, refs_preds, average='macro')\n",
    "    print('Ref Rat. Acc: {:.4f}'.format(acc))\n",
    "    print('Ref Rat. F1: {:.4f}'.format(f1))\n",
    "    \n",
    "    acc = accuracy_score(edits_labels, edits_preds)\n",
    "    f1 = f1_score(edits_labels, edits_preds, average='macro')\n",
    "    print('Edit Rat. Acc: {:.4f}'.format(acc))\n",
    "    print('Edit Rat. F1: {:.4f}'.format(f1))\n",
    "    \n",
    "    print('---')\n",
    "    \n",
    "    y_pred = list(get_predictions(model, tokenizer, refs, verbose=False))\n",
    "    acc = accuracy_score(refs_labels, y_pred)\n",
    "    f1 = f1_score(refs_labels, y_pred, average='macro')\n",
    "    print('Ref Valid. Acc: {:.4f}'.format(acc))\n",
    "    print('Ref Valid. F1: {:.4f}'.format(f1))\n",
    "    \n",
    "    y_pred = list(get_predictions(model, tokenizer, edits, verbose=False))\n",
    "    acc = accuracy_score(edits_labels, y_pred)\n",
    "    f1 = f1_score(edits_labels, y_pred, average='macro')\n",
    "    print('Edit Valid. Acc: {:.4f}'.format(acc))\n",
    "    print('Edit Valid. F1: {:.4f}'.format(f1))\n",
    "    \n",
    "    y_pred = list(get_predictions(model, tokenizer, edits, verbose=False))\n",
    "    acc = accuracy_score(refs_labels, y_pred)\n",
    "    f1 = f1_score(refs_labels, y_pred, average='macro')\n",
    "    print('Edit Valid. Cont. Acc: {:.4f}'.format(1 - acc))\n",
    "    print('Edit Valid. Cont. F1: {:.4f}'.format(1 - f1))\n",
    "    \n",
    "    print('---')\n",
    "    \n",
    "    cleaned_refs = list(map(split_and_clean, refs))\n",
    "    cleaned_edits = list(map(split_and_clean, edits))\n",
    "    res = dist_ratio(get_tokenized_texts(cleaned_edits), get_tokenized_texts(cleaned_refs))\n",
    "    print('Levensh. dist: {:.2f}'.format(res))\n",
    "    res = np.mean(list(map(lambda x: len(x.split()), get_tokenized_texts(cleaned_refs))))\n",
    "    print('Num. tokens ref: {:.1f}'.format(res))\n",
    "    res = np.mean(list(map(lambda x: len(x.split()), get_tokenized_texts(cleaned_edits))))\n",
    "    print('Num. tokens edit: {:.1f}'.format(res))\n",
    "    \n",
    "    print('---')\n",
    "    \n",
    "    res = sacrebleu.compute(predictions=cleaned_edits, references=cleaned_refs)\n",
    "    print('Self-bleu: {:.2f}'.format(res['score']))\n",
    "    \n",
    "    print('---')\n",
    "    \n",
    "    cleaned_refs = list(map(clean, refs))\n",
    "    cleaned_edits = list(map(clean, edits))\n",
    "    res = perplexity.compute(predictions=cleaned_refs + cleaned_edits, model_id='gpt2-large')\n",
    "    perp_refs = res['perplexities'][:len(refs)]\n",
    "    perp_edits = res['perplexities'][len(refs):] \n",
    "    print('Ref Perpl: {:.2f}'.format(np.mean(perp_refs)))\n",
    "    print('Edit Perpl: {:.2f}'.format(np.mean(perp_edits)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "946af21f-aa10-4786-a7cb-32f211c5c43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ref Rat. Acc: 0.8989\n",
      "Ref Rat. F1: 0.6239\n",
      "Edit Rat. Acc: 0.7004\n",
      "Edit Rat. F1: 0.4924\n",
      "---\n",
      "Ref Valid. Acc: 0.9675\n",
      "Ref Valid. F1: 0.9674\n",
      "Edit Valid. Acc: 0.7545\n",
      "Edit Valid. F1: 0.7541\n",
      "Edit Valid. Cont. Acc: 0.7545\n",
      "Edit Valid. Cont. F1: 0.7547\n",
      "---\n",
      "Levensh. dist: 0.29\n",
      "Num. tokens ref: 7.5\n",
      "Num. tokens edit: 7.4\n",
      "---\n",
      "Self-bleu: 41.36\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1cb65a2b3224e83ad3038a2bf32a533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ref Perpl: 63.52\n",
      "Edit Perpl: 62.00\n"
     ]
    }
   ],
   "source": [
    "print_eval(f'../data/edits_paper/snli/crest_30p.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c067e93-8c53-4d6b-a931-ae57c77b3c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ref Rat. Acc: 0.8773\n",
      "Ref Rat. F1: 0.6134\n",
      "Edit Rat. Acc: 0.7581\n",
      "Edit Rat. F1: 0.5313\n",
      "---\n",
      "Ref Valid. Acc: 0.9675\n",
      "Ref Valid. F1: 0.9674\n",
      "Edit Valid. Acc: 0.8123\n",
      "Edit Valid. F1: 0.8123\n",
      "Edit Valid. Cont. Acc: 0.8123\n",
      "Edit Valid. Cont. F1: 0.8138\n",
      "---\n",
      "Levensh. dist: 0.41\n",
      "Num. tokens ref: 7.5\n",
      "Num. tokens edit: 7.3\n",
      "---\n",
      "Self-bleu: 30.53\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66a1352a4849402b9e6082b2a3656d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ref Perpl: 63.52\n",
      "Edit Perpl: 62.60\n"
     ]
    }
   ],
   "source": [
    "print_eval(f'../data/edits_paper/snli/crest_50p.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e241aa66-bfa6-4923-b204-0b8c48dac0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ref Rat. Acc: 0.8845\n",
      "Ref Rat. F1: 0.6155\n",
      "Edit Rat. Acc: 0.7653\n",
      "Edit Rat. F1: 0.5364\n",
      "---\n",
      "Ref Valid. Acc: 0.9675\n",
      "Ref Valid. F1: 0.9674\n",
      "Edit Valid. Acc: 0.7617\n",
      "Edit Valid. F1: 0.7617\n",
      "Edit Valid. Cont. Acc: 0.7617\n",
      "Edit Valid. Cont. F1: 0.7637\n",
      "---\n",
      "Levensh. dist: 0.35\n",
      "Num. tokens ref: 7.5\n",
      "Num. tokens edit: 7.9\n",
      "---\n",
      "Self-bleu: 42.18\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edda8afa2dc946a987b911e40a561e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ref Perpl: 63.52\n",
      "Edit Perpl: 63.19\n"
     ]
    }
   ],
   "source": [
    "print_eval(f'../data/edits_paper/snli/mice_binary_search.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5783a64-c36e-4958-acd1-5c952194da3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ref Rat. Acc: 0.8845\n",
      "Ref Rat. F1: 0.6155\n",
      "Edit Rat. Acc: 0.7148\n",
      "Edit Rat. F1: 0.5056\n",
      "---\n",
      "Ref Valid. Acc: 0.9675\n",
      "Ref Valid. F1: 0.9674\n",
      "Edit Valid. Acc: 0.7726\n",
      "Edit Valid. F1: 0.7726\n",
      "Edit Valid. Cont. Acc: 0.7726\n",
      "Edit Valid. Cont. F1: 0.7748\n",
      "---\n",
      "Levensh. dist: 0.40\n",
      "Num. tokens ref: 7.5\n",
      "Num. tokens edit: 8.3\n",
      "---\n",
      "Self-bleu: 34.08\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cea9a6f486494f4c96e538cca3d1bded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ref Perpl: 63.52\n",
      "Edit Perpl: 59.71\n"
     ]
    }
   ],
   "source": [
    "print_eval(f'../data/edits_paper/snli/mice_30p.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d76a632-de5a-45dc-bc55-648c13c8d9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ref Rat. Acc: 0.8845\n",
      "Ref Rat. F1: 0.6155\n",
      "Edit Rat. Acc: 0.7906\n",
      "Edit Rat. F1: 0.5509\n",
      "---\n",
      "Ref Valid. Acc: 0.9675\n",
      "Ref Valid. F1: 0.9674\n",
      "Edit Valid. Acc: 0.8448\n",
      "Edit Valid. F1: 0.8447\n",
      "Edit Valid. Cont. Acc: 0.8448\n",
      "Edit Valid. Cont. F1: 0.8496\n",
      "---\n",
      "Levensh. dist: 0.52\n",
      "Num. tokens ref: 7.5\n",
      "Num. tokens edit: 7.6\n",
      "---\n",
      "Self-bleu: 24.27\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "443652e2948c4389839c3225e9a82ae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ref Perpl: 63.52\n",
      "Edit Perpl: 68.32\n"
     ]
    }
   ],
   "source": [
    "print_eval(f'../data/edits_paper/snli/mice_50p.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f54d36f-96e3-43e8-94d4-2240686fb4c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
